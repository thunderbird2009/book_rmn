{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7babe27d",
   "metadata": {},
   "source": [
    "# Test Code for Appendix 5: Embedding Architectures\n",
    "\n",
    "This notebook tests the code listings from `ap5_embeddings.md` to ensure they are functional and consistent with the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a9755",
   "metadata": {},
   "source": [
    "## 1. Fundamentals of Neural Embeddings\n",
    "\n",
    "Testing the basic embedding lookup example from Section 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba74859c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for Product 42: tensor([-1.0160, -1.3530, -0.4644,  0.8269, -0.0637], grad_fn=<SliceBackward0>)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define embedding table: 10,000 product IDs → 128-d vectors\n",
    "product_embedding = nn.Embedding(num_embeddings=10000, embedding_dim=128)\n",
    "\n",
    "# Input: batch of product IDs\n",
    "product_ids = torch.tensor([42, 156, 7890, 42])  # shape: (4,)\n",
    "\n",
    "# Lookup embeddings\n",
    "embeddings = product_embedding(product_ids)  # shape: (4, 128)\n",
    "\n",
    "print(f\"Embedding for Product 42: {embeddings[0][:5]}...\")  # First 5 dims\n",
    "# Output: tensor([-0.1234,  0.5678, -0.9012,  0.3456, -0.7890], grad_fn=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9049b4c",
   "metadata": {},
   "source": [
    "## 2. Sequential Embeddings for Text\n",
    "\n",
    "Testing Code Listing A5.1: Query Encoder with Positional Embeddings from Section 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7be33d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class QueryEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size=30000, embed_dim=128, max_seq_len=64, num_heads=4, num_layers=4):\n",
    "        super().__init__()\n",
    "        # Token embeddings (learned)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional embeddings (learned)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=embed_dim*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_ids: (batch_size, seq_len) - tokenized queries\n",
    "        Returns:\n",
    "            query_embedding: (batch_size, embed_dim) - fixed-size query vectors\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = token_ids.shape\n",
    "        \n",
    "        # Token embeddings: (batch_size, seq_len, embed_dim)\n",
    "        token_embeds = self.token_embedding(token_ids)\n",
    "        \n",
    "        # Positional embeddings: (seq_len, embed_dim)\n",
    "        positions = torch.arange(seq_len, device=token_ids.device)\n",
    "        pos_embeds = self.pos_embedding(positions).unsqueeze(0)  # (1, seq_len, embed_dim)\n",
    "        \n",
    "        # Combine: (batch_size, seq_len, embed_dim)\n",
    "        input_embeds = token_embeds + pos_embeds\n",
    "        \n",
    "        # Create padding mask (0 = valid token, 1 = padding)\n",
    "        # Assumes token_id=0 is padding token\n",
    "        padding_mask = (token_ids == 0)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Transformer encoding: (batch_size, seq_len, embed_dim)\n",
    "        contextualized = self.transformer(input_embeds, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # Average pooling (excluding padding tokens): (batch_size, embed_dim)\n",
    "        mask_expanded = (~padding_mask).unsqueeze(-1).float()  # (batch_size, seq_len, 1)\n",
    "        masked_contextualized = contextualized * mask_expanded\n",
    "        query_embedding = masked_contextualized.sum(dim=1) / mask_expanded.sum(dim=1).clamp(min=1)\n",
    "        \n",
    "        return query_embedding\n",
    "\n",
    "# Example usage\n",
    "encoder = QueryEncoder(vocab_size=30000, embed_dim=128)\n",
    "query_tokens = torch.tensor([[1523, 4201, 5678, 89, 9432]])  # \"best family hotel in Orlando\"\n",
    "query_embed = encoder(query_tokens)\n",
    "print(f\"Query embedding shape: {query_embed.shape}\")  # torch.Size([1, 128])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d36d78",
   "metadata": {},
   "source": [
    "## 3. Sequential Embeddings for Multi-Dimensional Behavioral Actions\n",
    "\n",
    "Testing Code Listing A5.2: Behavioral Sequence Encoder with Temporal Decay from Section 3.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3712f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavioral embedding shape: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BehavioralSequenceEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes user behavioral sequences with temporal decay.\n",
    "    Input: sequence of (action_id, item_id, context_id, timestamp) tuples\n",
    "    Output: fixed-size behavioral embedding vector\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 action_vocab_size=50,\n",
    "                 item_vocab_size=1_000_000,\n",
    "                 context_vocab_size=500,\n",
    "                 action_emb_dim=32,\n",
    "                 item_emb_dim=64,\n",
    "                 context_emb_dim=32,\n",
    "                 pos_emb_dim=32,\n",
    "                 hidden_dim=256,\n",
    "                 decay_rate=0.02):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding tables\n",
    "        self.action_emb = nn.Embedding(action_vocab_size, action_emb_dim, padding_idx=0)\n",
    "        self.item_emb = nn.Embedding(item_vocab_size, item_emb_dim, padding_idx=0)\n",
    "        self.context_emb = nn.Embedding(context_vocab_size, context_emb_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(50, pos_emb_dim)  # max sequence length 50\n",
    "        \n",
    "        # Temporal decay\n",
    "        self.decay_rate = decay_rate\n",
    "        \n",
    "        # Concatenated dimension: 32 + 64 + 32 + 32 = 160\n",
    "        concat_dim = action_emb_dim + item_emb_dim + context_emb_dim + pos_emb_dim\n",
    "        \n",
    "        # DIN-style attention network\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(concat_dim * 3, 128),  # [action, ad, action*ad]\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Ad embedding projection to match action vector dimension\n",
    "        self.ad_proj = nn.Linear(hidden_dim, concat_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        self.output_proj = nn.Linear(concat_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, action_ids, item_ids, context_ids, time_deltas, candidate_ad_emb):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action_ids: [batch_size, seq_len] - action type tokens\n",
    "            item_ids: [batch_size, seq_len] - item ID tokens\n",
    "            context_ids: [batch_size, seq_len] - context tokens\n",
    "            time_deltas: [batch_size, seq_len] - seconds since current request\n",
    "            candidate_ad_emb: [batch_size, hidden_dim] - current ad embedding\n",
    "        Returns:\n",
    "            behavioral_emb: [batch_size, hidden_dim] - user intent embedding\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = action_ids.shape\n",
    "        \n",
    "        # Lookup embeddings\n",
    "        action_emb = self.action_emb(action_ids)  # [B, L, 32]\n",
    "        item_emb = self.item_emb(item_ids)        # [B, L, 64]\n",
    "        context_emb = self.context_emb(context_ids)  # [B, L, 32]\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len, device=action_ids.device))  # [L, 32]\n",
    "        pos_emb = pos_emb.unsqueeze(0).expand(batch_size, -1, -1)  # [B, L, 32]\n",
    "        \n",
    "        # Concatenate all components\n",
    "        action_vec = torch.cat([action_emb, item_emb, context_emb, pos_emb], dim=-1)  # [B, L, 160]\n",
    "        \n",
    "        # Compute temporal decay weights\n",
    "        temporal_weights = torch.exp(-self.decay_rate * time_deltas)  # [B, L]\n",
    "        temporal_weights = temporal_weights.unsqueeze(-1)  # [B, L, 1]\n",
    "        \n",
    "        # DIN attention: compare each action to candidate ad\n",
    "        ad_emb_expanded = candidate_ad_emb.unsqueeze(1).expand(-1, seq_len, -1)  # [B, L, hidden_dim]\n",
    "        \n",
    "        # Project ad embedding to match action_vec dimension\n",
    "        ad_proj = self.ad_proj(ad_emb_expanded)  # [B, L, 160]\n",
    "        \n",
    "        # Attention input: [action, ad, action*ad]\n",
    "        # Why this design? Element-wise product captures feature interactions (e.g., \n",
    "        # \"user clicked laptops\" × \"current ad is laptop\" → high similarity signal)\n",
    "        attn_input = torch.cat([\n",
    "            action_vec, \n",
    "            ad_proj, \n",
    "            action_vec * ad_proj\n",
    "        ], dim=-1)  # [B, L, 480]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attn_scores = self.attention_net(attn_input).squeeze(-1)  # [B, L]\n",
    "        \n",
    "        # Softmax over sequence to get base attention weights\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1).unsqueeze(-1)  # [B, L, 1]\n",
    "        \n",
    "        # Apply temporal decay to attention weights\n",
    "        # Recent actions keep full weight, older actions get dampened\n",
    "        attn_weights = attn_weights * temporal_weights\n",
    "        \n",
    "        # Weighted sum\n",
    "        behavioral_vec = (action_vec * attn_weights).sum(dim=1)  # [B, 160]\n",
    "        \n",
    "        # Final projection\n",
    "        behavioral_emb = self.output_proj(behavioral_vec)  # [B, 256]\n",
    "        \n",
    "        return behavioral_emb\n",
    "\n",
    "# Test the BehavioralSequenceEncoder\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "hidden_dim = 256\n",
    "\n",
    "# Create dummy inputs\n",
    "action_ids = torch.randint(1, 50, (batch_size, seq_len))\n",
    "item_ids = torch.randint(1, 1000, (batch_size, seq_len))\n",
    "context_ids = torch.randint(1, 500, (batch_size, seq_len))\n",
    "time_deltas = torch.rand((batch_size, seq_len)) * 100  # 0 to 100 seconds\n",
    "candidate_ad_emb = torch.randn((batch_size, hidden_dim))\n",
    "\n",
    "# Initialize model\n",
    "model = BehavioralSequenceEncoder(\n",
    "    action_vocab_size=50,\n",
    "    item_vocab_size=1000,\n",
    "    context_vocab_size=500,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = model(action_ids, item_ids, context_ids, time_deltas, candidate_ad_emb)\n",
    "print(f\"Behavioral embedding shape: {output.shape}\")  # Should be [4, 256]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485dd11",
   "metadata": {},
   "source": [
    "## 4. Rigorous Testing\n",
    "\n",
    "We will now perform deeper validation to ensure the specific logic (Padding, Temporal Decay, DIN Attention) works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becea97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 1: Padding Masking Verification ---\n",
      "Difference between [A, B] and [A, B, PAD]: 0.000000\n",
      "SUCCESS: Padding is correctly masked out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Documents\\code\\my_books\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test 1: Padding Masking Verification ---\")\n",
    "# Create two inputs: one with padding, one without (but same valid tokens)\n",
    "# Note: To compare exactly, we need to handle the fact that QueryEncoder \n",
    "# expects a batch. We'll compare [A, B] vs [A, B, PAD]\n",
    "# But wait, the model has learned positional embeddings. \n",
    "# [A, B] has positions 0, 1.\n",
    "# [A, B, PAD] has positions 0, 1, 2.\n",
    "# If masking works, A and B attend only to 0, 1. PAD is ignored.\n",
    "# So A and B states should be identical.\n",
    "# Pooling sums A and B and divides by 2.\n",
    "# So the final vector should be identical.\n",
    "\n",
    "vocab_size = 30000\n",
    "encoder = QueryEncoder(vocab_size=vocab_size, embed_dim=128)\n",
    "encoder.eval() # Set to eval mode\n",
    "\n",
    "# Token IDs\n",
    "token_A = 100\n",
    "token_B = 200\n",
    "pad_token = 0\n",
    "\n",
    "# Input 1: [A, B] padded to length 3 -> [A, B, PAD]\n",
    "input_padded = torch.tensor([[token_A, token_B, pad_token]])\n",
    "\n",
    "# Input 2: [A, B] (length 2)\n",
    "input_clean = torch.tensor([[token_A, token_B]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_padded = encoder(input_padded)\n",
    "    out_clean = encoder(input_clean)\n",
    "\n",
    "# Check difference\n",
    "diff = (out_padded - out_clean).abs().max().item()\n",
    "print(f\"Difference between [A, B] and [A, B, PAD]: {diff:.6f}\")\n",
    "if diff < 1e-5:\n",
    "    print(\"SUCCESS: Padding is correctly masked out.\")\n",
    "else:\n",
    "    print(\"FAILURE: Padding affects the result!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f796ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test 2: Temporal Decay & DIN Attention Verification ---\n",
      "Effect of temporal decay (0s vs 100s): 0.708817\n",
      "SUCCESS: Temporal decay changes the output embedding.\n",
      "\n",
      "--- Test 3: DIN Attention Verification ---\n",
      "Effect of changing candidate ad: 0.050368\n",
      "SUCCESS: DIN Attention adapts to candidate ad.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Test 2: Temporal Decay & DIN Attention Verification ---\")\n",
    "# Initialize model\n",
    "hidden_dim = 256\n",
    "model = BehavioralSequenceEncoder(hidden_dim=hidden_dim)\n",
    "model.eval()\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 2\n",
    "\n",
    "# Fixed inputs\n",
    "action_ids = torch.tensor([[1, 2]]) # Action 1, Action 2\n",
    "item_ids = torch.tensor([[10, 20]])\n",
    "context_ids = torch.tensor([[5, 5]])\n",
    "\n",
    "# Case A: No time decay (both recent)\n",
    "time_recent = torch.tensor([[0.0, 0.0]])\n",
    "\n",
    "# Case B: Action 1 is old, Action 2 is recent\n",
    "time_decay = torch.tensor([[100.0, 0.0]]) # 100s gap\n",
    "\n",
    "# Candidate Ad (random)\n",
    "candidate_ad = torch.randn(batch_size, hidden_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_recent = model(action_ids, item_ids, context_ids, time_recent, candidate_ad)\n",
    "    out_decay = model(action_ids, item_ids, context_ids, time_decay, candidate_ad)\n",
    "\n",
    "# Check if time affects output\n",
    "time_diff = (out_recent - out_decay).abs().max().item()\n",
    "print(f\"Effect of temporal decay (0s vs 100s): {time_diff:.6f}\")\n",
    "if time_diff > 1e-4:\n",
    "    print(\"SUCCESS: Temporal decay changes the output embedding.\")\n",
    "else:\n",
    "    print(\"FAILURE: Temporal decay has no effect.\")\n",
    "\n",
    "print(\"\\n--- Test 3: DIN Attention Verification ---\")\n",
    "# Check if changing the candidate ad changes the output (Attention should shift)\n",
    "candidate_ad_2 = torch.randn(batch_size, hidden_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_ad1 = model(action_ids, item_ids, context_ids, time_recent, candidate_ad)\n",
    "    out_ad2 = model(action_ids, item_ids, context_ids, time_recent, candidate_ad_2)\n",
    "\n",
    "ad_diff = (out_ad1 - out_ad2).abs().max().item()\n",
    "print(f\"Effect of changing candidate ad: {ad_diff:.6f}\")\n",
    "if ad_diff > 1e-4:\n",
    "    print(\"SUCCESS: DIN Attention adapts to candidate ad.\")\n",
    "else:\n",
    "    print(\"FAILURE: Candidate ad has no effect (Attention might be broken).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
